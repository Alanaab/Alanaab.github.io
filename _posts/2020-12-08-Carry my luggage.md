# Carry my luggage

[TOC]

### 1. 简介

设计一个帮助人从购物中心到停车场搬运行李的机器人，在很少人为干预的前提下跟踪服务对象，能够跟随人行走从而将行李送往目的地，从而让旅客更加省时省力。具体如下： 

- 假设使用者正面面对人，其身边左右两侧放有两个不同的包（测试时将采用书包、双肩包、拎包、手提包等） 
- 假设机器人可以完全看清人的完整上半身（包括指点手势），即允许测试人员调整站姿直到完整看清为止。通过 kinect 骨架提取检测静态指向手势，通过方向范围判 断出人所指明的物体。 
- 采用物体检测器，在物体上画出bounding box，假设背景干净。若机器人看不清物体，可以前进仔细观察，但需要保证停下时物体大致位于画面中心、不丢失目标。 
- 跟随被服务的对象，直到他到达其车位旁（模拟为场地中的某个设定目标点）





### 2. 项目描述

#### 2.1 总体描述

本项目主要制作一个实验性行李搬运机器人系统，我们将该产品的功能分为如下三个模块： 

- 行李定位 
- 行李模拟抓取 
- 身份识别和跟踪行驶 

依靠上述三个基本模块，借助于Turtlebot 机器人平台，一个较为完整的智能行李搬运机器人系统就能够被搭建出来。在经过实体集成测试和性能评估后，该系统可以被小范围投入使用，并进行相应的市场调研与用户体验分析。

#### 2.2 具体分析 

1. 为了保证机器人能够得知需要帮助搬运的行李的位置，旅客首先要用手指出行李所在位置，利用手势识别程序，使得机器人接收到需要搬运行李的消息。在此之后，利用目标 检测程序进行物体框定和目标选取，精准定位货物的位置和大小，以便之后的机械臂成 功进行货物抓取。 
2. 为了让机器人能够成功抓取货物，我们必须使用机械臂，并对结合给定的目标位置和行李大小等条件，对标定后的机械臂进行自动操控，将抓取到的行李放到自己的货架上。 对于一款成熟的行李搬运机器人来说，应当具备行李重量感应装置与行李丢失报警装置， 防止行李的意外丢失造成的财产纠纷与其他不必要的麻烦。 
3. 在实际的应用场景中，这一功能可以通过识别旅客的身份证、护照、车票等实现，本次系统设计采取较为简单的方法，利用人体检测模型直接定位人体所在位置，并在识别到人体后，利用深度信息判断旅客是否正在移动，如果正在移动，则进行自动跟随。





### 3. 过程总结

在本次项目中，主要的工作内容有跟踪、二维图片下的骨架提取以及系统集成和其他一 些小工作。

#### 3.1 Tracking

跟踪部分大致可以分为基于特征和基于深度学方法两类，调研了不少现有的跟踪算法如下：

![image](https://github.com/Alanaab/Alanaab.github.io/raw/master/img/CML_Tracking_Methods.png)

##### 3.1.1 KCF

KCF 是对 MOSSE 的改进，把以前只能用单通道的灰度特征改进为现在可以使用多通 道的 HOG 特征或者其他特征，而且在现有算法中是表现比较好的，使用 HOG 替换掉了灰 度特征，对实验部分进行了扩充。使用核函数，对偶相关滤波去计算。

**1) 原理：**主要是对预选框进行滤波，信号输出响应最大的位置，便认为是目标所在位置

![image](https://github.com/Alanaab/Alanaab.github.io/raw/master/img/CML_KCF_Method.png)

**2) 工作过程：**左边图是刚开始我们使用红色虚线框框定了目标，然后红色实线框就是使用 的 padding 了，其他的框就是将 padding 循环移位之后对齐目标得到的样本，由这些样 本就可以训练出一个分类器，当分类器设计好之后，来到了下一帧图像，也就是右图，这 时候我们首先在预测区域也就是红色实线框区域采样，然后对该采样进行循环移位，对 齐目标后就像图中显示的那个样子，使用分类器对这些框框计算响应，显然这时候白色 框响应最大，因为他和之前一帧红色框一样，那我们通过白色框的相对移位就能推测目标的位移了。

**3) 优点：**

- 由于滤波运算过程中还是延续 MOSSE，用到了 FFT，所以速度快得以继承。
- 把以前只能用单通道的灰度特征改进为现在可以使用多通道的 HOG 特征或者其他 特征，鲁棒性更强。
- 原理简单，易懂，易上手。

**4) 缺点：**

- 对非刚体运动跟踪适应能力弱，但目标发生形变（如人站着变成蹲下）的时候，预 选定框误差会很大。
- 难以处理高速运动物体，因为 KCF 是在 padding 框内进行信号筛选的，如果物体 运动过快，在下一帧超出了 padding 框，将无法预测。
- 同样，当物体丢失时，也无法重跟踪。

![image](https://github.com/Alanaab/Alanaab.github.io/raw/master/img/CML_KCF_Method_2.png)

针对于本次跟踪任务，帧率只需要保证在 20 帧以上即可很好完成任务，而基于 MOSSE 特征的跟踪算法，虽然可轻松达上百帧，这是冗余的，其次考虑到跟踪过程中，人很可 能会蹲下、因受阻挡或者运动稍快而超出预选框的情况。考虑到以上几点，决定尝试一 下其他跟踪算法，在保证有 20 帧以上的同时，又能不受预选框的束缚。

##### 3.1.2 ATOM

基于 MOSSE 特征的方法都有以上提到的共同缺点，要想不受预选框等束缚，决定尝 试一下基于深度学习的方法。查阅近几年在 Tracking 方面的论文后，挑选了一篇 backbone 比较浅的 ATOM。作者为苏黎世联邦理工大学的 Martin Danelljan.

**1)原理**

- 目标跟踪划分为了目标定位和形状估计两个子任务，并提出了两个模块来实现。分 别为一个离线训练的目标估计模块，一个在线训练的目标分类（定位）模块。这两 个任务被融合进了一个统一的网络结构。如下图：

![image](https://github.com/Alanaab/Alanaab.github.io/raw/master/img/CML_ATOM_Network.png)

上面是 reference branch，也就是目标模板的分支，下面是 test branch，也就是当前 帧的分支。在模板分支，经过 backbone 提取特征之后，将特征经过”IoU Modulation” 模块编码成了一个“调制向量”，用于后面每一帧 bounding box 的 iou 预测。下面 这个分支中，当前帧的图像经过 backbone 后分成了一个分类分支和一个 iou 预测 分支，分类分支用于初步定位目标，得到初始的 bounding box。iou 预测分支结合 前面生成的调制向量，推断 bounding box 的 iou，并经过反向传播，优化 bounding box 使得 iou 最大化，以此得到精细的预测框。橙色的 resnet-18 直接使用预训练模 型，不经过 fine-tune；蓝色的模块为目标估计模块，是经过线下训练得到的；绿色 的为在线训练的目标分类模块，也就是说在 tracking 的时候才进行训练。

**2）目标估计网络**

- 其实就是使用了 iou-net 的最大化 iou 来调整 bounding box 的思想

![image](https://github.com/Alanaab/Alanaab.github.io/raw/master/img/CML_Estimation.png)

- reference branch 通过 resnet-18 作为 backbone 提取特征，然后对目标区域进行 PrPool (iou-net 里的)，最后通过全连接层编码成两个 modulation vector。这里只 需要进行一次，也就是初始化的时候需要，后面推断的时候就只需要用到保存下来 的 modulation vector 了。
- test brach 也是先用 resnet-18 对当前帧提取特征，而后对 bounding box 进行 PrPool 得到固定尺寸的特征图。这里的 bounding box 是 classifier 那边得来的一个大概的 框，后面讲那个部分的时候会讲是怎么生成的。
- 使用 reference branch 生成的两个 vector 分别对两个特征图进行 channel-wise multiplication，也就相当于 channel 维度上的 attention。 
- 通过几个全连接层得到预测的 iou。 
- 对 iou 求 bounding box 的梯度，经过几次迭代，调整 bounding box 最大化 iou，得 到精细的预测框。

**3) 分类网络**

传统深度学习的梯度下降或者随机梯度下降的收敛速度都比较慢，它们不适 合在线的深度学习 training。作者自己提出了一套基于共轭梯度和牛顿高斯的方法进行优化。

**4) Tracking 流程**

1. 对于第一帧，模板分支产生调制向量，快速训练 classifier。
2. 对于当前帧，提取特征，经过 classifier 确定位置，再结合前一帧的 bounding box 形 状，生成当前帧的初始 bounding box。 
3. 将 bounding box 传入 iou predictor 产生预测的 iou。 
4. 通过最大化 iou，来优化 bounding box，经过几次迭代产生当前帧的预测框。 
5. 预测下一帧，至步骤 2。需要说明的是，步骤 2 中，作者发现根据位置信息，结合随 机的形状生成的多个初始化 bounding box 能够更好的避免局部最优。这样，也就是 通过优化多个 bbox，并取出 iou 最高的三个 bbox，取平均得到最后的预测。

##### 3.1.3 整体流程

-  获取深度图和 RGB 图。 
- 利用 RGB 图送至 ATOM，跟踪二维信息 
- 选取对应框中点的 10*10 个像素点，利用深度信息，计算平均深度 
- 选取框中心的 X 坐标和深度信息作为控制量 
- 使用 PD 控制，分别控制 Turtlebot 的 angle 和 LinearSpeed。

#### 3.2 集成

本次项目一共分为了四个模块，分别是

- 手势识别
- 点云识别背包提取
- 跟踪人体
- Turtlebot 运动控制

各个模块之间使用 ROS 相互通信，流程图如下。

![image](https://github.com/Alanaab/Alanaab.github.io/raw/master/img/CML_Estimation_2.png)



### 4. 遇到的问题

-  prthon3 中 cv-bridge 问题：本次跟踪使用的是 Python3，由于 ROS 自带 cv-bridge 用 python2 编译，暂时没有解决 python3 中不能使用 Ros 自带的 cv-bridge 的问题， 导致图像无法通过 ROS 传输到跟踪线程。解决方案是在跟踪线程中主动打开摄像头 获取 RGB 图像和深度图像。 
- ATOM 在线训练机制： ATOM 算法在本次任务中有一个缺点，由于它的分类网络 是在线训练的，因此，开始的一段时间内会出现跟踪不够稳的情况。并且如果有一个 特征相似的人出现的画面中，而同时跟踪目标一段时间不在画面内，可能会导致分类 网络去适应了另一个干扰人，导致正真的目标人即使再次进入画面，也无法快速识别出来。
- 本次项目是通过提取 2D 图像下的骨架，映射到 3D 点云信息中，由于 3D 点云是离散的，可能会出现映射到 3 维，点不存在或者是点不匹配的现象。目前解决方法是通过深度 信息筛选出合理的映射点，但可能有一定的偏差。



### 5. 总体评价

#### 5.1 Tracking

硬件设备：显卡为 GTX1050，i7 酷睿处理器。单独测试 ATOM 跟踪算法时，帧率约 为 25 帧，达到本次项目的要求，并且经过测试，很好避开了基于 MOSSE 特征的缺点，帧 间差异大也能实现很好跟踪。对跟踪模块的总体性能比较满意。

#### 5.2 点云识别背包

传统方法对自然环境要求比较高，相机的角度或者高度发生变化，都会影响背包的识 别，鲁棒性较差，而且处理时间很长。

#### 5.3 骨架提取

无论是使用 iai 库还是二维的姿态识别网络，效果都很好。本次项目主要以方法探索为 主，整体运行速度、鲁棒性等与实际应用还有一定距离。
